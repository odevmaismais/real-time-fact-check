import { GoogleGenAI, LiveServerMessage, Modality } from "@google/genai";
import { AnalysisResult, VerdictType } from "../types";

const MODEL_NAME = "gemini-2.0-flash-exp";
const LIVE_MODEL_NAME = "models/gemini-2.0-flash-exp";

// --- TIPOS E ESTADOS ---

export type LiveStatus = {
Â  type: 'info' | 'warning' | 'error';
Â  message: string;
};

type ConnectionState = 'DISCONNECTED' | 'CONNECTING' | 'CONNECTED' | 'RECONNECTING';

export interface LiveConnectionController {
Â  Â  disconnect: () => Promise<void>;
}

// --- AUDIO WORKLET CODE (INLINE) ---
// Mantido o seu cÃ³digo original que FUNCIONA (Box Filter + Tanh Boost)
// Isso garante volume suficiente para o VAD do Gemini ativar.
const PCM_PROCESSOR_CODE = `
class PCMProcessor extends AudioWorkletProcessor {
Â  constructor() {
Â  Â  super();
Â  Â  this.buffer = new Int16Array(4096);Â 
Â  Â  this.bufferIndex = 0;
Â  Â  this.targetRate = 16000;
Â  }

Â  process(inputs, outputs, parameters) {
Â  Â  const input = inputs[0];
Â  Â  if (!input || !input[0]) return true;
Â  Â Â 
Â  Â  const inputChannel = input[0];
Â  Â  const ratio = sampleRate / this.targetRate;
Â  Â Â 
Â  Â  let inputIndex = 0;
Â  Â Â 
Â  Â  while (inputIndex < inputChannel.length) {
Â  Â  Â  Â  let sum = 0;
Â  Â  Â  Â  let count = 0;
Â  Â  Â  Â Â 
Â  Â  Â  Â  const start = Math.floor(inputIndex);
Â  Â  Â  Â  const end = Math.min(inputChannel.length, Math.floor(inputIndex + ratio));
Â  Â  Â  Â Â 
Â  Â  Â  Â  for (let i = start; i < end; i++) {
Â  Â  Â  Â  Â  Â  sum += inputChannel[i];
Â  Â  Â  Â  Â  Â  count++;
Â  Â  Â  Â  }
Â  Â  Â  Â Â 
Â  Â  Â  Â  if (count === 0 && start < inputChannel.length) {
Â  Â  Â  Â  Â  Â  sum = inputChannel[start];
Â  Â  Â  Â  Â  Â  count = 1;
Â  Â  Â  Â  }

Â  Â  Â  Â  const avg = count > 0 ? sum / count : 0;
Â  Â  Â  Â Â 
Â  Â  Â  Â  // BOOST: Essencial para o Gemini ouvir o Ã¡udio
Â  Â  Â  Â  const boosted = Math.tanh(avg * 2.5);Â 
Â  Â  Â  Â Â 
Â  Â  Â  Â  const pcm = boosted < 0 ? boosted * 0x8000 : boosted * 0x7FFF;
Â  Â  Â  Â Â 
Â  Â  Â  Â  if (this.bufferIndex >= this.buffer.length) {
Â  Â  Â  Â  Â  Â  this.port.postMessage(this.buffer.slice(0, this.bufferIndex));
Â  Â  Â  Â  Â  Â  this.bufferIndex = 0;
Â  Â  Â  Â  }
Â  Â  Â  Â Â 
Â  Â  Â  Â  this.buffer[this.bufferIndex++] = pcm;
Â  Â  Â  Â  inputIndex += ratio;
Â  Â  }
Â  Â  return true;
Â  }
}
registerProcessor('pcm-processor', PCMProcessor);
`;

// --- UTILS ---

// ATENÃ‡ÃƒO: Esta funÃ§Ã£o remove espaÃ§os extras. SÃ³ deve ser usada na exibiÃ§Ã£o final,
// nunca na concatenaÃ§Ã£o do buffer de stream.
const cleanTranscriptText = (text: string): string => {
Â  if (!text) return "";
Â  return text.replace(/\s+/g, ' ').trim();
};

function arrayBufferToBase64(buffer: ArrayBuffer | SharedArrayBuffer): string {
Â  Â  let binary = '';
Â  Â  const bytes = new Uint8Array(buffer);
Â  Â  const len = bytes.byteLength;
Â  Â  for (let i = 0; i < len; i++) {
Â  Â  Â  Â  binary += String.fromCharCode(bytes[i]);
Â  Â  }
Â  Â  return window.btoa(binary);
}

// --- FACT CHECKING (Mantido) ---
export const analyzeStatement = async (
Â  text: string,
Â  segmentId: string,
Â  contextHistory: string[] = []Â 
): Promise<AnalysisResult> => {
Â  const apiKey = process.env.GEMINI_API_KEY || process.env.API_KEY;
Â  if (!apiKey) throw new Error("API Key is missing");
Â  const ai = new GoogleGenAI({ apiKey });
Â Â 
Â  try {
Â  Â  const prompt = `
Â  Â  Â  CONTEXTO: Checagem de fatos (Brasil).
Â  Â  Â  CONTEXTO ANTERIOR: ${contextHistory.join(" | ")}
Â  Â  Â  FRASE: "${text}"
Â  Â  Â  TAREFA: Classificar e verificar.
Â  Â  Â  Retorne JSON: { verdict: "TRUE"|"FALSE"|"MISLEADING"|"OPINION", explanation: "...", confidence: 0.9 }
Â  Â  `;

Â  Â  const response = await ai.models.generateContent({
Â  Â  Â  model: MODEL_NAME,
Â  Â  Â  contents: prompt,
Â  Â  Â  config: {
Â  Â  Â  Â  tools: [{ googleSearch: {} }],
Â  Â  Â  Â  responseMimeType: "application/json",
Â  Â  Â  },
Â  Â  });

Â  Â  const jsonText = response.text || "{}";Â 
Â  Â  const data = JSON.parse(jsonText);
Â  Â  const sources = response.candidates?.[0]?.groundingMetadata?.groundingChunks
Â  Â  Â  ?.map((chunk: any) => chunk.web)
Â  Â  Â  .filter((web: any) => web && web.uri && web.title) || [];

Â  Â  return {
Â  Â  Â  segmentId,
Â  Â  Â  verdict: data.verdict as VerdictType || VerdictType.UNVERIFIABLE,
Â  Â  Â  confidence: data.confidence || 0,
Â  Â  Â  explanation: data.explanation || "Sem anÃ¡lise",
Â  Â  Â  counterEvidence: data.counterEvidence,
Â  Â  Â  sources: sources,
Â  Â  Â  sentimentScore: data.sentimentScore || 0,
Â  Â  Â  logicalFallacies: data.logicalFallacies || [],
Â  Â  Â  context: contextHistory,
Â  Â  Â  tokenUsage: {
Â  Â  Â  Â  Â  promptTokens: response.usageMetadata?.promptTokenCount || 0,
Â  Â  Â  Â  Â  responseTokens: response.usageMetadata?.candidatesTokenCount || 0,
Â  Â  Â  Â  Â  totalTokens: response.usageMetadata?.totalTokenCount || 0
Â  Â  Â  }
Â  Â  };
Â  } catch (error) {
Â  Â  console.error("Erro anÃ¡lise:", error);
Â  Â  return {
Â  Â  Â  segmentId,
Â  Â  Â  verdict: VerdictType.UNVERIFIABLE,
Â  Â  Â  confidence: 0,
Â  Â  Â  explanation: "Erro tÃ©cnico.",
Â  Â  Â  sources: [],
Â  Â  Â  sentimentScore: 0,
Â  Â  Â  logicalFallacies: [],
Â  Â  Â  context: [],
Â  Â  };
Â  }
};

// --- CORE LIVE CONNECTION ---

export const connectToLiveDebate = async (
Â  originalStream: MediaStream,
Â  onTranscript: (data: { text: string; speaker: string; isFinal: boolean }) => void,
Â  onError: (err: Error) => void,
Â  onStatus?: (status: LiveStatus) => void
): Promise<LiveConnectionController> => {
Â  const apiKey = process.env.GEMINI_API_KEY || process.env.API_KEY;
Â  if (!apiKey) {
Â  Â  onError(new Error("API Key missing"));
Â  Â  return { disconnect: async () => {} };
Â  }

Â  const stream = originalStream.clone();
Â  const ai = new GoogleGenAI({ apiKey });

Â  let connectionState: ConnectionState = 'DISCONNECTED';
Â  let shouldMaintainConnection = true;
Â Â 
Â  let activeSessionPromise: Promise<any> | null = null;
Â  let audioContext: AudioContext | null = null;
Â  let workletNode: AudioWorkletNode | null = null;
Â  let sourceNode: MediaStreamAudioSourceNode | null = null;
Â  let reconnectTimeout: any = null;

Â  const initAudioStack = async () => {
Â  Â  Â  try {
Â  Â  Â  Â  Â  audioContext = new AudioContext();Â 
Â  Â  Â  Â  Â  if (audioContext.state === 'suspended') await audioContext.resume();

Â  Â  Â  Â  Â  console.log(`ðŸ”Š AudioContext iniciado em ${audioContext.sampleRate}Hz`);

Â  Â  Â  Â  Â  const blob = new Blob([PCM_PROCESSOR_CODE], { type: "application/javascript" });
Â  Â  Â  Â  Â  const workletUrl = URL.createObjectURL(blob);
Â  Â  Â  Â  Â  await audioContext.audioWorklet.addModule(workletUrl);

Â  Â  Â  Â  Â  sourceNode = audioContext.createMediaStreamSource(stream);
Â  Â  Â  Â  Â  workletNode = new AudioWorkletNode(audioContext, 'pcm-processor');

Â  Â  Â  Â  Â  workletNode.port.onmessage = (event) => {
Â  Â  Â  Â  Â  Â  Â  if (connectionState === 'CONNECTED') {
Â  Â  Â  Â  Â  Â  Â  Â  Â  const pcmInt16 = event.data;
Â  Â  Â  Â  Â  Â  Â  Â  Â  sendAudioChunk(pcmInt16);
Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  };

Â  Â  Â  Â  Â  sourceNode.connect(workletNode);
Â  Â  Â  Â  Â  workletNode.connect(audioContext.destination);Â 
Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  console.log("ðŸ”Š Audio Worklet Initialized");

Â  Â  Â  } catch (e) {
Â  Â  Â  Â  Â  console.error("Falha ao iniciar Audio Engine", e);
Â  Â  Â  Â  Â  onError(e as Error);
Â  Â  Â  }
Â  };

Â  const sendAudioChunk = (pcmInt16: Int16Array) => {
Â  Â  Â  if (connectionState !== 'CONNECTED' || !activeSessionPromise) return;

Â  Â  Â  const base64Data = arrayBufferToBase64(pcmInt16.buffer);

Â  Â  Â  activeSessionPromise.then(async (session) => {
Â  Â  Â  Â  Â  if (connectionState !== 'CONNECTED') return;
Â  Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  Â  Â  // Envia objeto formatado corretamente { media: ... }
Â  Â  Â  Â  Â  Â  Â  await session.sendRealtimeInput({Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  media: {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mimeType: "audio/pcm;rate=16000",Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data: base64Data
Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  });
Â  Â  Â  Â  Â  } catch (e: any) {
Â  Â  Â  Â  Â  Â  Â  if (e.message && (e.message.includes("CLOSING") || e.message.includes("CLOSED"))) return;
Â  Â  Â  Â  Â  Â  Â  console.warn("Tx Error:", e);
Â  Â  Â  Â  Â  }
Â  Â  Â  }).catch(() => {});
Â  };

Â  const establishConnection = async () => {
Â  Â  if (!shouldMaintainConnection) return;

Â  Â  connectionState = 'CONNECTING';
Â  Â  onStatus?.({ type: 'info', message: "CONECTANDO..." });

Â  Â  try {
Â  Â  Â  Â  const sessionPromise = ai.live.connect({
Â  Â  Â  Â  Â  model: LIVE_MODEL_NAME,Â 
Â  Â  Â  Â  Â  config: {
Â  Â  Â  Â  Â  Â  responseModalities: [Modality.TEXT],Â 
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  // @ts-ignore
Â  Â  Â  Â  Â  Â  inputAudioTranscription: { },Â 
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  systemInstruction: {
Â  Â  Â  Â  Â  Â  Â  Â  parts: [{ text: "Transcreva o Ã¡udio para PortuguÃªs do Brasil (PT-BR) imediatamente. TranscriÃ§Ã£o verbatim: palavra por palavra." }]
Â  Â  Â  Â  Â  Â  },
Â  Â  Â  Â  Â  },
Â  Â  Â  Â  Â  callbacks: {
Â  Â  Â  Â  Â  Â  onopen: () => {
Â  Â  Â  Â  Â  Â  Â  Â console.log("ðŸŸ¢ Conectado (Mode: TEXT, Transc: ON)");
Â  Â  Â  Â  Â  Â  Â  Â connectionState = 'CONNECTED';
Â  Â  Â  Â  Â  Â  Â  Â onStatus?.({ type: 'info', message: "ONLINE" });
Â  Â  Â  Â  Â  Â  },
Â  Â  Â  Â  Â  Â  onmessage: (msg: LiveServerMessage) => {
Â  Â  Â  Â  Â  Â  Â  Â const inputTranscript = msg.serverContent?.inputTranscription?.text;
Â  Â  Â  Â  Â  Â  Â  Â const modelText = msg.serverContent?.modelTurn?.parts?.[0]?.text;
Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â if (inputTranscript) handleText(inputTranscript);
Â  Â  Â  Â  Â  Â  Â  Â if (modelText) handleText(modelText);
Â  Â  Â  Â  Â  Â  },
Â  Â  Â  Â  Â  Â  onclose: (e) => {
Â  Â  Â  Â  Â  Â  Â  Â console.log(`ðŸ”´ Socket Fechado (${e.code})`);
Â  Â  Â  Â  Â  Â  Â  Â connectionState = 'DISCONNECTED';
Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â if (e.code === 1000) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â shouldMaintainConnection = false;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â onStatus?.({ type: 'info', message: "Desconectado" });
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â return;
Â  Â  Â  Â  Â  Â  Â  Â }

Â  Â  Â  Â  Â  Â  Â  Â if (shouldMaintainConnection) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â connectionState = 'RECONNECTING';
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â onStatus?.({ type: 'warning', message: "RECONECTANDO..." });
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â reconnectTimeout = setTimeout(establishConnection, 1000);Â 
Â  Â  Â  Â  Â  Â  Â  Â }
Â  Â  Â  Â  Â  Â  },
Â  Â  Â  Â  Â  Â  onerror: (err) => {
Â  Â  Â  Â  Â  Â  Â  Â  console.error("Erro Socket:", err);
Â  Â  Â  Â  Â  Â  Â  Â  connectionState = 'DISCONNECTED';
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  }
Â  Â  Â  Â  });

Â  Â  Â  Â  activeSessionPromise = sessionPromise;
Â  Â  Â  Â  sessionPromise.catch(() => {
Â  Â  Â  Â  Â  Â  Â if (shouldMaintainConnection && connectionState !== 'CONNECTED') {
Â  Â  Â  Â  Â  Â  Â  Â  Â reconnectTimeout = setTimeout(establishConnection, 1000);
Â  Â  Â  Â  Â  Â  Â }
Â  Â  Â  Â  });

Â  Â  } catch (err) {
Â  Â  Â  Â  connectionState = 'DISCONNECTED';
Â  Â  Â  Â  if (shouldMaintainConnection) reconnectTimeout = setTimeout(establishConnection, 1000);
Â  Â  }
Â  };

Â  // --- CORREÃ‡ÃƒO DO PICOTADO (TEXT HANDLING) ---
Â  let currentBuffer = "";
Â Â 
Â  const handleText = (raw: string) => {
Â  Â  Â  // 1. NÃƒO usamos cleanTranscriptText(raw) aqui.
Â  Â  Â  // O Gemini envia pedaÃ§os como " ca" (com espaÃ§o) ou "sa" (sem espaÃ§o) que colam perfeitamente.
Â  Â  Â  // Se limparmos antes, perdemos a cola.
Â  Â  Â Â 
Â  Â  Â  if (raw) {
Â  Â  Â  Â  Â  console.log("ðŸ“ Chunk Puro:", `"${raw}"`); // Debug para ver os espaÃ§os
Â  Â  Â  Â  Â  currentBuffer += raw;Â 
Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  // 2. Enviamos para UI com trim() apenas visual
Â  Â  Â  Â  Â  // O `onTranscript` da UI pode fazer o que quiser, mas o `currentBuffer` mantÃ©m a integridade
Â  Â  Â  Â  Â  onTranscript({ text: currentBuffer.trim(), speaker: "DEBATE", isFinal: false });
Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  // 3. DetecÃ§Ã£o de fim de frase
Â  Â  Â  Â  Â  if (currentBuffer.length > 150 || raw.match(/[.!?]$/)) {
Â  Â  Â  Â  Â  Â  Â  onTranscript({ text: currentBuffer.trim(), speaker: "DEBATE", isFinal: true });
Â  Â  Â  Â  Â  Â  Â  currentBuffer = "";
Â  Â  Â  Â  Â  }
Â  Â  Â  }
Â  };

Â  await initAudioStack();Â 
Â  establishConnection();Â 

Â  return {
Â  Â  Â  Â disconnect: async () => {
Â  Â  Â  Â  Â  Â console.log("ðŸ›‘ Encerrando SessÃ£o...");
Â  Â  Â  Â  Â  Â shouldMaintainConnection = false;
Â  Â  Â  Â  Â  Â connectionState = 'DISCONNECTED';
Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â if (reconnectTimeout) clearTimeout(reconnectTimeout);

Â  Â  Â  Â  Â  Â if (workletNode) {
Â  Â  Â  Â  Â  Â  Â  Â workletNode.port.onmessage = null;
Â  Â  Â  Â  Â  Â  Â  Â workletNode.disconnect();
Â  Â  Â  Â  Â  Â }
Â  Â  Â  Â  Â  Â if (sourceNode) sourceNode.disconnect();
Â  Â  Â  Â  Â  Â if (audioContext && audioContext.state !== 'closed') await audioContext.close();

Â  Â  Â  Â  Â  Â if (activeSessionPromise) {
Â  Â  Â  Â  Â  Â  Â  Â try {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â const session = await activeSessionPromise;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â await session.close();
Â  Â  Â  Â  Â  Â  Â  Â } catch (e) { /* ignore */ }
Â  Â  Â  Â  Â  Â }
Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â stream.getTracks().forEach(t => t.stop());Â 
Â  Â  Â  Â }
Â  Â  };
}
